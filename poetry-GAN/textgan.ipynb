{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.imports import *\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/.zshenv:1: bad assignment\n",
      "# packages in environment at /Users/rchaks/opt/anaconda3/envs/aml-proj:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "fastai                    1.0.61                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('../data/')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    train_sentences, valid_sentences = train_test_split(sentences, test_size= 0.1, random_state = 121)\n",
    "\n",
    "    data_lm = TextLMDataBunch.from_df(path = path, train_df= sentences, valid_df= valid_sentences, text_cols= \"0\")\n",
    "    return data_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your man on the road, he doin promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You said, \"Keep our business on the low-low\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Im just tryna get you out the friend zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cause you look even better than the photos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I cant find your house, send me the info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40819</th>\n",
       "      <td>Scene goes frame by frame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40820</th>\n",
       "      <td>Who swayed to suede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40821</th>\n",
       "      <td>Youll hear it all dont call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40822</th>\n",
       "      <td>Throw away everything Ive written you oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40823</th>\n",
       "      <td>Anything just like every single thing todayEmb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40824 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                    Your man on the road, he doin promo\n",
       "1           You said, \"Keep our business on the low-low\"\n",
       "2              Im just tryna get you out the friend zone\n",
       "3             Cause you look even better than the photos\n",
       "4               I cant find your house, send me the info\n",
       "...                                                  ...\n",
       "40819                          Scene goes frame by frame\n",
       "40820                                Who swayed to suede\n",
       "40821                        Youll hear it all dont call\n",
       "40822           Throw away everything Ive written you oh\n",
       "40823  Anything just like every single thing todayEmb...\n",
       "\n",
       "[40824 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.read_csv(\"../data/lyrics_preprocessed.csv\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(decode_error='ignore'\n",
    "#                             # ,ngram_range = (1,2)\n",
    "#                             ,use_idf = True\n",
    "#                             ,strip_accents = 'ascii'\n",
    "#                             # ,smooth_idf = False\n",
    "#                             )\n",
    "# song_vectors = vectorizer.fit_transform(sentences[\"0\"])\n",
    "# print(song_vectors.shape)\n",
    "# # 7042 -> Training instances\n",
    "# # 21954 -> Total words\n",
    "# print(min(vectorizer.idf_), max(vectorizer.idf_))\n",
    "# idf_vocab = pd.Series(vectorizer.idf_, index= vectorizer.get_feature_names())\n",
    "\n",
    "# # Selecting top 10% most frequently occouring words\n",
    "# # Selecting top 10% most frequently occouring words\n",
    "# vocab = idf_vocab[idf_vocab <= idf_vocab.quantile(q = 0.15)]\n",
    "# print(idf_vocab.quantile(q = 0.2), len(vocab))\n",
    "\n",
    "# vocab = pd.Series([t for t in vocab.index if not t.isnumeric()], dtype=str)\n",
    "# vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(decode_error='ignore'\n",
    "#                             # ,ngram_range = (1,2)\n",
    "#                             ,strip_accents = 'ascii'\n",
    "#                             # ,smooth_idf = False\n",
    "#                             ,vocabulary = set(vocab)\n",
    "#                         )\n",
    "# X = vectorizer.fit_transform(sentences[\"0\"])\n",
    "# print(X.shape)\n",
    "# # 7042 -> Training instances\n",
    "# # 21954 -> Total words\n",
    "# vocab= vectorizer.get_feature_names_out()\n",
    "# print(vocab)\n",
    "# print(X.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/opt/anaconda3/envs/aml-proj/lib/python3.9/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = load_data(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = data_lm.train_dl\n",
    "val_dl = data_lm.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_drop_lin(n_in, n_out, bn=True, initrange=0.01,p=0, bias=True, actn=nn.LeakyReLU(inplace=True)):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    linear = nn.Linear(n_in, n_out, bias=bias)\n",
    "    if initrange:linear.weight.data.uniform_(-initrange, initrange)\n",
    "    if bias: linear.bias.data.zero_()\n",
    "    layers.append(linear)\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_drop_lin(n_in, n_out, bn=True, initrange=0.01,p=0, bias=True, actn=nn.LeakyReLU(inplace=True)):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    linear = nn.Linear(n_in, n_out, bias=bias)\n",
    "    if initrange:linear.weight.data.uniform_(-initrange, initrange)\n",
    "    if bias: linear.bias.data.zero_()\n",
    "    layers.append(linear)\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/opt/anaconda3/envs/aml-proj/lib/python3.9/site-packages/fastai/text/learner.py:215: UserWarning: There are no pretrained weights for that architecture yet!\n",
      "  warn(\"There are no pretrained weights for that architecture yet!\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.text.learner import language_model_learner\n",
    "from fastai.text.models import TransformerXL\n",
    "learn = language_model_learner(data_lm, arch=TransformerXL)\n",
    "# learn.load('lyrics_fine_tuned_novel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerXL(\n",
       "  (encoder): Embedding(5264, 410)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = deepcopy(learn.model[0])\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trn_dl))\n",
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/opt/anaconda3/envs/aml-proj/lib/python3.9/site-packages/fastai/text/models/transformer.py:114: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1634272478997/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n",
      "  attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n"
     ]
    }
   ],
   "source": [
    "outs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70, 410])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[-1][-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([64, 70, 410])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[out.size() for out in outs[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = deepcopy(learn.model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.load_state_dict(learn.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextDicriminator(nn.Module):\n",
    "    def __init__(self,encoder, nh, bn_final=True):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        self.encoder = encoder\n",
    "        #classifier\n",
    "        layers = []\n",
    "        layers+=bn_drop_lin(nh*3,nh,bias=False)\n",
    "        layers += bn_drop_lin(nh,nh,p=0.25)\n",
    "        layers+=bn_drop_lin(nh,1,p=0.15,actn=nn.Sigmoid())\n",
    "        if bn_final: layers += [nn.BatchNorm1d(1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(0,2,1), (1,)).view(bs,-1)\n",
    "    \n",
    "    def forward(self, inp,y=None):\n",
    "        raw_outputs, outputs = self.encoder(inp)\n",
    "        output = outputs[-1]\n",
    "        bs,sl,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[:,-1], mxpool, avgpool], 1)\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = TextDicriminator(encoder,400)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(disc.parameters(), lr = 3e-4)\n",
    "optimizerG = optim.Adam(generator.parameters(), lr = 3e-3, betas=(0.7, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seq_gumbel_softmax(input):\n",
    "    samples = []\n",
    "    bs,sl,nc = input.size()\n",
    "    for i in range(sl): \n",
    "        z = F.gumbel_softmax(input[:,i,:])\n",
    "        samples.append(torch.multinomial(z,1))\n",
    "    samples = torch.stack(samples).transpose(1,0).squeeze(2) \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reinforce_loss(input,sample,reward):\n",
    "    loss=0\n",
    "    bs,sl = sample.size()\n",
    "    for i in range(sl):\n",
    "        loss += -input[:,i,sample[:,i]] * reward\n",
    "    return loss/sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def step_gen(ds,gen,disc,optG,crit=None):\n",
    "    gen.train(); disc.train()\n",
    "    x,y = ds\n",
    "    bs,sl = x.size()\n",
    "    fake,_,_ = gen(x)\n",
    "    gen.zero_grad()\n",
    "    fake_sample =seq_gumbel_softmax(fake)\n",
    "    with torch.no_grad():\n",
    "        gen_loss = reward = disc(fake_sample)\n",
    "        if crit: gen_loss = crit(fake,fake_sample,reward.squeeze(1))\n",
    "        gen_loss = gen_loss.mean()\n",
    "    gen_loss.requires_grad_(True)\n",
    "    gen_loss.backward()\n",
    "    optG.step()\n",
    "    return gen_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def step_disc(ds,gen,disc,optD,d_iters):\n",
    "    for j in range(d_iters):\n",
    "        gen.eval(); disc.train()\n",
    "        with torch.no_grad():\n",
    "            fake,_,_ = gen(x)\n",
    "            fake_sample = seq_gumbel_softmax(fake)\n",
    "        disc.zero_grad()\n",
    "        fake_loss = disc(fake_sample)\n",
    "        real_loss = disc(y.view(bs,sl))\n",
    "        disc_loss = (fake_loss-real_loss).mean(0)\n",
    "        disc_loss.backward()\n",
    "        optimizerD.step()\n",
    "    return disc_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(ds,gen,disc,crit=None):\n",
    "    with torch.no_grad():\n",
    "        x, y = ds\n",
    "        bs,sl = x.size()\n",
    "        fake,_,_ = gen(x)\n",
    "        fake_sample =seq_gumbel_softmax(fake)\n",
    "        gen_loss = reward = disc(fake_sample)\n",
    "        if crit: gen_loss = crit(fake,fake_sample,reward.squeeze(1))\n",
    "        gen_loss = gen_loss.mean()\n",
    "        fake_sample = seq_gumbel_softmax(fake)\n",
    "        fake_loss = disc(fake_sample).mean(0).view(1).data.item()\n",
    "        real_loss = disc(y.view(bs,sl)).mean(0).view(1).data.item()\n",
    "        disc_loss = (fake_loss-real_loss).mean(0).view(1).data.item()\n",
    "    return fake,gen_loss,disc_loss,fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(gen, disc, epochs, trn_dl, val_dl, optimizerD, optimizerG, crit=None,first=True):\n",
    "    gen_iterations = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        gen.train(); disc.train()\n",
    "        n = len(trn_dl)\n",
    "        #train loop\n",
    "        with tqdm(total=n) as pbar:\n",
    "            for i, ds in enumerate(trn_dl):\n",
    "                gen_loss = step_gen(ds,gen,disc,optimizerG,crit)\n",
    "                gen_iterations += 1\n",
    "                d_iters = 3\n",
    "                disc_loss = step_disc(ds,gen,disc,optimizerD,d_iters)\n",
    "                pbar.update()\n",
    "        print(f'Epoch {epoch}:')\n",
    "        print('Train Loss:')\n",
    "        print(f'Loss_D {disc_loss}; Loss_G {gen_loss} Ppx {torch.exp(lm_loss(fake,y))}')\n",
    "        print(f'D_real {real_loss}; Loss_D_fake {fake_loss}')\n",
    "        \n",
    "        disc.eval(), gen.eval()\n",
    "        with tqdm(total=len(val_dl)) as pbar:\n",
    "            for i, ds in enumerate(val_dl):\n",
    "                fake,gen_loss,disc_loss,fake_loss = evaluate(ds,gen,disc,crit)\n",
    "                pbar.update()\n",
    "        print('Valid Loss:')\n",
    "        print(f'Loss_D {disc_loss}; Loss_G {gen_loss} Ppx {torch.exp(lm_loss(fake,ds[-1]))}')\n",
    "        print(f'D_real {real_loss}; Loss_D_fake {fake_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nh = {'AWD':400,'XL':410}\n",
    "crits={'gumbel':None,'reinforce':reinforce_loss}\n",
    "\n",
    "    learn = language_model_learner(data_lm, arch=TransformerXL)\n",
    "    learn.load(pretrained)\n",
    "    encoder = deepcopy(learn.model[0])\n",
    "    \n",
    "    generator = deepcopy(learn.model)\n",
    "    generator.load_state_dict(learn.model.state_dict())\n",
    "    disc = TextDicriminator(encoder,nh[model]).cuda()\n",
    "    \n",
    "    disc.train()\n",
    "    generator.train()\n",
    "    \n",
    "    #create optimizers\n",
    "    optimizerD = optim.Adam(disc.parameters(), lr = 3e-4)\n",
    "    optimizerG = optim.Adam(generator.parameters(), lr = 3e-3, betas=(0.7, 0.8))\n",
    "    \n",
    "    print(f'training for {epochs} epochs')\n",
    "    train(generator, disc, epochs, trn_dl, val_dl, optimizerD, optimizerG, first=False)\n",
    "    \n",
    "    #save model\n",
    "    learn.model.load_state_dict(generator.state_dict())\n",
    "    print(f'saving model to {path}/{filename}_{model}_gan_{crit}')\n",
    "    learn.save(filename+'_'+model+'_gan_'+crit)\n",
    "    \n",
    "    #generate output from validation set\n",
    "    if preds:\n",
    "        print(f'generating predictions and saving to {path}/{filename}_{model}_preds.txt;')\n",
    "        get_valid_preds(learn,data_lm,filename+'_'+model+'_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# nh = {'AWD':400,'XL':410}\n",
    "# crits={'gumbel':None,'reinforce':reinforce_loss}\n",
    "\n",
    "# #train a language model with gan objective\n",
    "# def run(path,filename,pretrained,model,crit=None,preds=True,epochs=6):\n",
    "#     #load data after running preprocess\n",
    "#     print(f'loading data from {path}/{filename};')\n",
    "#     data_lm = load_data(path, filename)\n",
    "#     trn_dl = data_lm.train_dl\n",
    "#     val_dl = data_lm.valid_dl\n",
    "    \n",
    "#     #select encoder for model\n",
    "#     print(f'training text gan model {model}; pretrained from {pretrained};')\n",
    "#     learn = language_model_learner(data_lm, arch=models[model])\n",
    "#     learn.load(pretrained)\n",
    "#     encoder = deepcopy(learn.model[0])\n",
    "    \n",
    "#     generator = deepcopy(learn.model)\n",
    "#     generator.load_state_dict(learn.model.state_dict())\n",
    "#     disc = TextDicriminator(encoder,nh[model]).cuda()\n",
    "    \n",
    "#     disc.train()\n",
    "#     generator.train()\n",
    "    \n",
    "#     #create optimizers\n",
    "#     optimizerD = optim.Adam(disc.parameters(), lr = 3e-4)\n",
    "#     optimizerG = optim.Adam(generator.parameters(), lr = 3e-3, betas=(0.7, 0.8))\n",
    "    \n",
    "#     print(f'training for {epochs} epochs')\n",
    "#     train(generator, disc, epochs, trn_dl, val_dl, optimizerD, optimizerG, first=False)\n",
    "    \n",
    "#     #save model\n",
    "#     learn.model.load_state_dict(generator.state_dict())\n",
    "#     print(f'saving model to {path}/{filename}_{model}_gan_{crit}')\n",
    "#     learn.save(filename+'_'+model+'_gan_'+crit)\n",
    "    \n",
    "#     #generate output from validation set\n",
    "#     if preds:\n",
    "#         print(f'generating predictions and saving to {path}/{filename}_{model}_preds.txt;')\n",
    "#         get_valid_preds(learn,data_lm,filename+'_'+model+'_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mERROR: \u001b[0mThe function received no value for the required argument: path\n",
      "Usage: ipykernel_launcher.py PATH FILENAME PRETRAINED MODEL <flags>\n",
      "  optional flags:        --crit | --preds | --epochs\n",
      "\n",
      "For detailed information on this command, run:\n",
      "  ipykernel_launcher.py --help\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mFireExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/opt/anaconda3/envs/aml-proj/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "if __name__ == '__main__': fire.Fire(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !notebook2script.py textgan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/.zshenv:1: bad assignment\n",
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbconvert` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script textgan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
